{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\pysol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (2.3.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\pysol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (0.18.1)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\pysol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (2.3.1)\n",
      "Requirement already satisfied: torchmetrics in c:\\users\\pysol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (1.4.0.post0)\n",
      "Requirement already satisfied: filelock in c:\\users\\pysol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\pysol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\pysol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\pysol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\pysol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\pysol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from torch) (2024.2.0)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\pysol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from torch) (2021.4.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\pysol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from torchvision) (1.25.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\pysol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from torchvision) (9.5.0)\n",
      "Requirement already satisfied: packaging>17.1 in c:\\users\\pysol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from torchmetrics) (23.1)\n",
      "Requirement already satisfied: lightning-utilities>=0.8.0 in c:\\users\\pysol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from torchmetrics) (0.11.5)\n",
      "Requirement already satisfied: setuptools in c:\\program files\\windowsapps\\pythonsoftwarefoundation.python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\site-packages (from lightning-utilities>=0.8.0->torchmetrics) (65.5.0)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\pysol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\pysol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.13.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\pysol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\pysol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from sympy->torch) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.1.2\n",
      "[notice] To update, run: C:\\Users\\pysol\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch torchvision torchaudio torchmetrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Skipping transformers as it is not installed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Obtaining dependency information for transformers from https://files.pythonhosted.org/packages/6a/dc/23c26b7b0bce5aaccf2b767db3e9c4f5ae4331bd47688c1f2ef091b23696/transformers-4.42.4-py3-none-any.whl.metadata\n",
      "  Using cached transformers-4.42.4-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\pysol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\pysol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from transformers) (0.24.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17 in c:\\users\\pysol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from transformers) (1.25.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\pysol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\pysol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\pysol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from transformers) (2023.6.3)\n",
      "Requirement already satisfied: requests in c:\\users\\pysol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\pysol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\pysol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\pysol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\pysol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\pysol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.9.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\pysol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\pysol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests->transformers) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pysol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\pysol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pysol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests->transformers) (2023.5.7)\n",
      "Using cached transformers-4.42.4-py3-none-any.whl (9.3 MB)\n",
      "Installing collected packages: transformers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: 'C:\\\\Users\\\\pysol\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\\\LocalCache\\\\local-packages\\\\Python310\\\\site-packages\\\\transformers\\\\models\\\\deprecated\\\\trajectory_transformer\\\\convert_trajectory_transformer_original_pytorch_checkpoint_to_pytorch.py'\n",
      "HINT: This error might have occurred since this system does not have Windows Long Path support enabled. You can find information on how to enable this at https://pip.pypa.io/warnings/enable-long-paths\n",
      "\n",
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.1.2\n",
      "[notice] To update, run: C:\\Users\\pysol\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall transformers\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_defined_0': tensor(0.8896), 'quality': tensor(0.8896)}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://lightning.ai/docs/torchmetrics/stable/multimodal/clip_iqa.html\n",
    "import torch\n",
    "from torchmetrics.multimodal.clip_iqa import CLIPImageQualityAssessment\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "\"\"\"\n",
    "working\n",
    "torch==2.3.1\n",
    "torchvision==0.18.1\n",
    "torchmetrics==1.4.0.post0 \n",
    "huggingface-hub==0.24.0\n",
    "transformers==4.42.4\n",
    "\n",
    "\n",
    "not working\n",
    "transformers==4.43.1\n",
    "huggingface-hub==0.24.1\n",
    "torch==2.3.1\n",
    "torchaudio==2.3.1\n",
    "torchmetrics==1.4.0.post0\n",
    "torchvision==0.18.1\n",
    "\"\"\"\n",
    "\n",
    "def load_image(\n",
    "    image_path:str,\n",
    "    transforms:transforms.Compose\n",
    ")->torch.tensor:\n",
    "    # Open the image file\n",
    "    img = Image.open(image_path).convert('RGB')  # Convert to RGB\n",
    "    # display(img)\n",
    "    # display(img.size)\n",
    "    img_tensor = transforms(img)  # Apply the transformation\n",
    "    return img_tensor.unsqueeze(0)\n",
    "\n",
    "path = 'sample_images/Complex Imaging Tags _ Metal Marker Mfg.jpeg'\n",
    "_ = torch.manual_seed(42)\n",
    "# Define a transform to convert the image to a tensor and normalize it\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Converts the image to a PyTorch tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # Normalize for pre-trained models\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "img = load_image(path,transform)\n",
    "metric = CLIPImageQualityAssessment(\n",
    "    # model_name_or_path='openai/clip-vit-base-patch16',\n",
    "    prompts=((\"Good Photo.\", \"Bad Photo.\"), \"quality\")\n",
    ")\n",
    "metric(img)\n",
    "# /home/pysolver33/miniconda3/envs/vqaapi/bin/python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_defined_0': tensor(0.7124), 'quality': tensor(0.7124)}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'sample_images\\Complex Imaging Tags _ Metal Marker Mfg (1).jpeg'\n",
    "img = load_image(path,transform)\n",
    "metric = CLIPImageQualityAssessment(prompts=((\"Good Photo.\", \"Bad Photo.\"), \"quality\"))\n",
    "metric(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_defined_0': tensor(0.9425), 'quality': tensor(0.9425)}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'sample_images\\image (6).png'\n",
    "img = load_image(path,transform)\n",
    "metric = CLIPImageQualityAssessment(prompts=((\"Good Photo.\", \"Bad Photo.\"), \"quality\"))\n",
    "metric(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_defined_0': tensor(0.6709), 'quality': tensor(0.6709)}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'sample_images\\IMG_9821.jpeg'\n",
    "img = load_image(path,transform)\n",
    "metric = CLIPImageQualityAssessment(prompts=((\"Good Photo.\", \"Bad Photo.\"), \"quality\"))\n",
    "metric(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Will watch for changes in these directories: ['c:\\\\Users\\\\pysol\\\\Desktop\\\\projects\\\\openai-quickstart-fastapi']\n",
      "INFO:     Uvicorn running on http://localhost:5001 (Press CTRL+C to quit)\n",
      "INFO:     Started reloader process [9444] using WatchFiles\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "from openai import OpenAI\n",
    "import uvicorn\n",
    "import base64\n",
    "import requests\n",
    "import json\n",
    "from fastapi import FastAPI, Request, Form,UploadFile,File\n",
    "from fastapi.responses import HTMLResponse, RedirectResponse,JSONResponse,RedirectResponse\n",
    "from fastapi.templating import Jinja2Templates\n",
    "from fastapi.staticfiles import StaticFiles\n",
    "\n",
    "from pydantic_settings import BaseSettings\n",
    "import io\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torchmetrics.multimodal.clip_iqa import CLIPImageQualityAssessment\n",
    "\n",
    "\n",
    "class Settings(BaseSettings):\n",
    "    OPENAI_API_KEY: str = 'OPENAI_API_KEY'\n",
    "    FLASK_APP: str = 'FLASK_APP'\n",
    "    FLASK_ENV: str = 'FLASK_ENV'\n",
    "    \n",
    "    class Config:\n",
    "        env_file = '.env'\n",
    "\n",
    "settings = Settings()\n",
    "client = OpenAI(api_key=settings.OPENAI_API_KEY)\n",
    "app = FastAPI()\n",
    "\n",
    "templates = Jinja2Templates(directory=\"templates\")\n",
    "app.mount(\"/static\", StaticFiles(directory=\"static\"), name=\"static\")\n",
    "\n",
    "# Function to encode the image\n",
    "def encode_image(image_path:str)->base64:\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "def get_json(\n",
    "    img_bytes:base64\n",
    ")->dict:\n",
    "    # Getting the base64 string\n",
    "    # base64_image = encode_image(file_name)\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {settings.OPENAI_API_KEY}\"\n",
    "    }\n",
    "\n",
    "    payload = {\n",
    "        \"model\": \"gpt-4o-mini\",\n",
    "        \"response_format\": {\"type\": \"json_object\"},\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"You are an intelligent system api that reads the texts from an image and outputs the texts as key values pairs in JSON format. Given an image, output the texts in JSON format.\"\n",
    "                },\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": f\"data:image/jpeg;base64,{img_bytes}\"\n",
    "                        }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "        \"max_tokens\": 1000\n",
    "    }\n",
    "\n",
    "    response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n",
    "    if response.status_code == 200:\n",
    "        # Save the response to a file\n",
    "        with open(\"response_data.json\", \"w\") as file:\n",
    "            file.write(response.text)\n",
    "    return response.json()\n",
    "    # return templates.TemplateResponse(\"json_out.html\", {\"request\": request, \"data\": response.json()})\n",
    "\n",
    "\n",
    "@app.get(\"/\", response_class=HTMLResponse)\n",
    "def index(request: Request):\n",
    "    return templates.TemplateResponse(\"upload_image.html\", {\"request\": request})\n",
    "\n",
    "\n",
    "@app.post(\"/output_iqa\")\n",
    "async def create_upload_file(\n",
    "    request:Request,\n",
    "    file: UploadFile = File(...),\n",
    "    choice: str = Form(...), \n",
    ")->dict:\n",
    "    pairs = {\n",
    "        'quality': (\"Good photo\", \"Bad photo\"),\n",
    "        'sharpness': (\"Sharp photo\", \"Blurry photo\"),\n",
    "        'noisiness': (\"Clean photo\", \"Noisy photo\"),\n",
    "    }\n",
    "    classes = pairs[choice.lower()]\n",
    "    \n",
    "    _ = torch.manual_seed(42)\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),  # Converts the image to a PyTorch tensor\n",
    "        # transforms.Normalize(mean=0,std=255)\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],  # Normalize for pre-trained models\n",
    "                            std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # Read the image file into a PIL Image\n",
    "    image_content = await file.read()\n",
    "    image = Image.open(io.BytesIO(image_content)).convert('RGB')\n",
    "    img = transform(image)\n",
    "    metric = CLIPImageQualityAssessment(\n",
    "        # model_name_or_path='openai/clip-vit-large-patch14',\n",
    "        prompts=(classes, choice)\n",
    "    )\n",
    "    out = {\n",
    "        'filename':file.filename,\n",
    "        'on':choice,\n",
    "        \"prompts\":classes,\n",
    "        \"score\": metric(img)[choice].item(),\n",
    "    }\n",
    "    if out['score'] > 0.75:\n",
    "        # return RedirectResponse(url=f'/get_json/?filename={file.filename}')\n",
    "        to_gpt = base64.b64encode(image.tobytes()).decode('utf-8')\n",
    "        info = get_json(to_gpt)\n",
    "        out.update(json.loads(info['choices'][0]['message']['content']))\n",
    "        return out\n",
    "    return out\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import logging\n",
    "    # NEED BELOW\n",
    "    # https://stackoverflow.com/questions/69774921/im-using-wsl-how-i-upgrade-python-to-the-last-version-through-the-console\n",
    "    # Disable uvicorn access logger\n",
    "    uvicorn_access = logging.getLogger(\"uvicorn.access\")\n",
    "    uvicorn_access.disabled = True\n",
    "\n",
    "    logger = logging.getLogger(\"uvicorn\")\n",
    "    logger.setLevel(logging.getLevelName(logging.DEBUG))\n",
    "    uvicorn.run('app:app', host=\"localhost\", port=5001, reload=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sid",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
